fit1 <- lm.ridge(Formula, data=x, lamdba=lambdas[i])
betas[, i] <- fit1$coef
scaledX <- sweep(as.matrix(x), 2, fit1$xm)
scaledX <- sweep(scaledX, 2, fit1$scale, "/")
yhat <- scaledX%*%fit1$coef + fit1$ym
test.rss[i] <- sum((y.test - yhat)^2)
}
plot(lambdas, test.rss, type="l", col="red", lwd=2, ylab="RSS", ylim=range(train.rss, test.rss))
lines(lambdas, train.rss, col="blue", lwd=2, lty=2)
best.lambda <- lambdas[which.min(test.rss)]
abline(v=best.lambda + 1/9)
legend(30, 30, c("Train", "Test"), col=c("blue", "red"), lty=c(2,1))
library(MASS)
lambdas <- seq(0, 50, len=10)
M <- length(lambdas)
train.rss <- rep(0, M)
test.rss <- rep(0, M)
betas <- matrix(0, ncol(x), M)
or(i in 1:M){
Formula <- as.formula(paste("y~", paste(covnames, collapse="+"), sep=""))
fit1 <- lm.ridge(Formula, data=x, lamdba=lambdas[i])
betas[, i] <- fit1$coef
scaledX <- sweep(as.matrix(x), 2, fit1$xm)
scaledX <- sweep(scaledX, 2, fit1$scale, "/")
yhat <- scaledX%*%fit1$coef + fit1$ym
test.rss[i] <- sum((y.test - yhat)^2)
}
for(i in 1:M){
Formula <- as.formula(paste("y~", paste(covnames, collapse="+"), sep=""))
fit1 <- lm.ridge(Formula, data=x, lamdba=lambdas[i])
betas[, i] <- fit1$coef
scaledX <- sweep(as.matrix(x), 2, fit1$xm)
scaledX <- sweep(scaledX, 2, fit1$scale, "/")
yhat <- scaledX%*%fit1$coef + fit1$ym
test.rss[i] <- sum((y.test - yhat)^2)
}
library(MASS)
lambdas <- seq(0,50,len=10)
M <- length(lambdas)
train.rss <- rep(0,M)
test.rss <- rep(0,M)
betas <- matrix(0,ncol(x),M)
for(i in 1:M){
Formula <-as.formula(paste("y~",paste(covnames,collapse="+"),sep=""))
fit1 <- lm.ridge(Formula,data=x,lambda=lambdas[i])
betas[,i] <- fit1$coef
scaledX <- sweep(as.matrix(x),2,fit1$xm)
scaledX <- sweep(scaledX,2,fit1$scale,"/")
yhat <- scaledX%*%fit1$coef+fit1$ym
train.rss[i] <- sum((y - yhat)^2)
scaledX <- sweep(as.matrix(x.test),2,fit1$xm)
scaledX <- sweep(scaledX,2,fit1$scale,"/")
yhat <- scaledX%*%fit1$coef+fit1$ym
test.rss[i] <- sum((y.test - yhat)^2)
}
png(file="Plots/selection-plots-02.png", width=432, height=432, pointsize=12)
plot(lambdas,test.rss,type="l",col="red",lwd=2,ylab="RSS",ylim=range(train.rss,test.rss))
lines(lambdas,train.rss,col="blue",lwd=2,lty=2)
best.lambda <- lambdas[which.min(test.rss)]
abline(v=best.lambda+1/9)
legend(30,30,c("Train","Test"),col=c("blue","red"),lty=c(2,1))
####
# regression subset selection in the prostate dataset
library(ElemStatLearn)
data(prostate)
covnames <- names(prostate[-(9:10)])
y <- prostate$lpsa
x <- prostate[,covnames]
form <- as.formula(paste("lpsa~", paste(covnames, collapse="+"), sep=""))
summary(lm(form, data=prostate[prostate$train,]))
set.seed(1)
train.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)
y.test <- prostate$lpsa[-train.ind]
x.test <- x[-train.ind,]
y <- prostate$lpsa[train.ind]
x <- x[train.ind,]
p <- length(covnames)
rss <- list()
for (i in 1:p) {
cat(i)
Index <- combn(p,i)
rss[[i]] <- apply(Index, 2, function(is) {
form <- as.formula(paste("y~", paste(covnames[is], collapse="+"), sep=""))
isfit <- lm(form, data=x)
yhat <- predict(isfit)
train.rss <- sum((y - yhat)^2)
yhat <- predict(isfit, newdata=x.test)
test.rss <- sum((y.test - yhat)^2)
c(train.rss, test.rss)
})
}
png("Plots/selection-plots-01.png", height=432, width=432, pointsize=12)
plot(1:p, 1:p, type="n", ylim=range(unlist(rss)), xlim=c(0,p), xlab="number of predictors", ylab="residual sum of squares", main="Prostate cancer data")
for (i in 1:p) {
points(rep(i-0.15, ncol(rss[[i]])), rss[[i]][1, ], col="blue")
points(rep(i+0.15, ncol(rss[[i]])), rss[[i]][2, ], col="red")
}
minrss <- sapply(rss, function(x) min(x[1,]))
lines((1:p)-0.15, minrss, col="blue", lwd=1.7)
minrss <- sapply(rss, function(x) min(x[2,]))
lines((1:p)+0.15, minrss, col="red", lwd=1.7)
legend("topright", c("Train", "Test"), col=c("blue", "red"), pch=1)
dev.off()
##
# ridge regression on prostate dataset
library(MASS)
lambdas <- seq(0,50,len=10)
M <- length(lambdas)
train.rss <- rep(0,M)
test.rss <- rep(0,M)
betas <- matrix(0,ncol(x),M)
for(i in 1:M){
Formula <-as.formula(paste("y~",paste(covnames,collapse="+"),sep=""))
fit1 <- lm.ridge(Formula,data=x,lambda=lambdas[i])
betas[,i] <- fit1$coef
scaledX <- sweep(as.matrix(x),2,fit1$xm)
scaledX <- sweep(scaledX,2,fit1$scale,"/")
yhat <- scaledX%*%fit1$coef+fit1$ym
train.rss[i] <- sum((y - yhat)^2)
scaledX <- sweep(as.matrix(x.test),2,fit1$xm)
scaledX <- sweep(scaledX,2,fit1$scale,"/")
yhat <- scaledX%*%fit1$coef+fit1$ym
test.rss[i] <- sum((y.test - yhat)^2)
}
png(file="Plots/selection-plots-02.png", width=432, height=432, pointsize=12)
plot(lambdas,test.rss,type="l",col="red",lwd=2,ylab="RSS",ylim=range(train.rss,test.rss))
lines(lambdas,train.rss,col="blue",lwd=2,lty=2)
best.lambda <- lambdas[which.min(test.rss)]
abline(v=best.lambda+1/9)
legend(30,30,c("Train","Test"),col=c("blue","red"),lty=c(2,1))
dev.off()
png(file="Plots/selection-plots-03.png", width=432, height=432, pointsize=8)
plot(lambdas,betas[1,],ylim=range(betas),type="n",ylab="Coefficients")
for(i in 1:ncol(x))
lines(lambdas,betas[i,],type="b",lty=i,pch=as.character(i))
abline(h=0)
legend("topright",covnames,pch=as.character(1:8))
dev.off()
#######
# lasso
library(lars)
lasso.fit <- lars(as.matrix(x), y, type="lasso", trace=TRUE)
png(file="Plots/selection-plots-04.png", width=432, height=432, pointsize=8)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
dev.off()
# this plots the cross validation curve
png(file="Plots/selection-plots-05.png", width=432, height=432, pointsize=12)
lasso.cv <- cv.lars(as.matrix(x), y, K=10, type="lasso", trace=TRUE)
dev.off()
library(lars)
install.packages("lars")
library(lars)
lasso.fit <- lars(as.matrix(x), y, type="lasso", trace=TRUE)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
png(width=432, height=432, pointsize=8)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
png(file = "plot.png", width=432, height=432, pointsize=8)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
dev.off()
install.packages("quantmod")
install.packages("swirlify")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
str(train)
str(SAheart)
fit <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(fit)
summary(fit)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missCLass(predict(fit, data.frame(ldl=c(0,1))))
missClass(predict(fit, data.frame(ldl=c(0,1))))
predict(fit, data.frame(ldl=c(0,1)))
predict(fit, trainSA$chd)
predict(fit, trainSA)
missClass(trainSA, predict(fit, trainSA))
missClass(trainSA, predict(fit, trainSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA, predict(fit, trainSA))
missClass()
missClass
missClass(trainSA, predict(fit, trainSA))
missClass(trainSA$chd, predict(fit, trainSA$chd))
missClass(trainSA$chd, predict(fit, trainSA))
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
### Function for misclassification rate
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
Answer:
### Setting up the logistic regression model
set.seed(13234)
fitTrain <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=trainSA)
fitTest <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=testSA)
### Calculating the misclassification error
missClass(trainSA$chd, predict(fitTrain, trainSA))
missClass(testSA$chd, predict(fitTest, testSA))
missClass(testSA$chd, predict(fitTrain, testSA))
set.seed(13234)
fitTrain <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=trainSA)
missClass(trainSA$chd, predict(fitTrain, trainSA))
missClass(testSA$chd, predict(fitTrain, testSA))
set.seed(13234)
fitTrain <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
### Calculating the misclassification error
missClass(trainSA$chd, predict(modelFit, trainSA))
missClass(testSA$chd, predict(modelFit, testSA))
library(kernlab)
set.seed(13234)
fitTrain <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
### Calculating the misclassification error
missClass(trainSA$chd, predict(modelFit, trainSA))
missClass(testSA$chd, predict(modelFit, testSA))
library(caret)
set.seed(13234)
fitTrain <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
### Calculating the misclassification error
missClass(trainSA$chd, predict(modelFit, trainSA))
missClass(testSA$chd, predict(modelFit, testSA))
summary(modelFit)
modelFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
### Calculating the misclassification error
missClass(trainSA$chd, predict(modelFit, trainSA))
missClass(testSA$chd, predict(modelFit, testSA))
randomForest
library(randomForest)
randomForest()
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.test)
str(vowel.train)
randomForest(y ~., vowel.train)
rf <- randomForest(y ~., vowel.train)
summary(rf)
qplot(rf)
plot(rf)
order(rf)
varImp(rf)
varImp
modelRF <- train(y ~., method="rf", data=vowel.train)
varImp(modelRF)
varImp(modelRF$pred)
?varImp
class(modelRF)
modelRF <- train(y ~., method="rf", data=vowel.train, importance=TRUE)
varImp(modelRF)
modelRF2 <- randomForest(y ~., data=vowel.train)
varImp(modelRF2)
summary(modelRF2)
modelRF <- train(y ~., method="rf", data=vowel.train, importance=TRUE)
varImp(modelRF, vowel.test)
varImp(vowel.test)
modelRF2 <- randomForest(y ~., data=vowel.test)
varImp(modelRF2)
?randomForest
modelRF2 <- randomForest(y ~., data=vowel.test, importance=TRUE)
varImp(modelRF2)
order(varImp(modelRF2))
order(desc(varImp(modelRF2)))
order(varImp(modelRF2))
str(vowel.test)
str(vowel.train)
combine <- rbind(vowel.train, vowel.test)
modelRF2 <- randomForest(y ~., data=combine, importance=TRUE)
order(varImp(modelRF2))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(ElemStatLearn)
library(rpart)
library(pgmm)
Answer:
set.seed(125)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.70, list=FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
modFit <- train(Case ~., method="rpart", data=training)
modFit$finalModel
z.auto <- rpart(Mileage ~ Weight, car.test.frame)
predict(z.auto)
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
predict(fit, type = "prob")   # class probabilities (default)
predict(fit, type = "vector") # level numbers
predict(fit, type = "class")  # factor
predict(fit, type = "matrix") # level number, class frequencies, probabilities
sub <- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
fit <- rpart(Species ~ ., data = iris, subset = sub)
fit
table(predict(fit, iris[-sub,], type = "class"), iris[-sub, "Species"])
predict(modelFit$finalModel)
predict(modelFit$finalModel, c(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2))
predict(modelFit$finalModel, c((TotalIntench2 = 23,000), (FiberWidthCh1 = 10), (PerimStatusCh1=2))
predict(modelFit$finalModel, c(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2))
predict(modelFit$finalModel, c((TotalIntench2 = 23000), (FiberWidthCh1 = 10), (PerimStatusCh1=2))
)
modelFit$finalModel
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.70, list=FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
modFit <- train(Case ~., method="rpart", data=training)
modFit$finalModel
predict(modFit$finalModel, c((TotalIntench2 = 23000), (FiberWidthCh1 = 10), (PerimStatusCh1=2))
)
modFit$finalModel
require(rCharts)
haireye = as.data.frame(HairEyeColor)
n1 <- nPlot(Freq ~ Hair, group = "Eye", type = "multiBarChart", data = subset(haireye, Sex == "Male"))
n1$save('fig/n1.html', cdn = TRUE)
cat('<iframe src = "fig/n1.html", width=100, height=600></iframe>')
nPlot
install.packages("plotly")
install.packages("plotly")
install.packages("plotly")
library(plotly)
library(plotly)
plot_ly(mtcars, x = wt, y = mpg, mode = "markers")
library(ggplot2)
library(ggplot)
library(ggplot2)
library(caret)
library(caret)
library(randomForest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
Answer:
set.seed(33833)
modelFit_rf <- randomForest(y ~., data=vowel.train)
modelFit_gbm <- train(y ~., data=vowel.train, method="gbm")
set.seed(33833)
modelFit_rf <- randomForest(as.factor(y) ~., data=vowel.train)
modelFit_gbm <- train(as.factor(y) ~., data=vowel.train, method="gbm")
summary(modelFit_gbm)
summary(modelFit_rf)
modelFit_rf$confusion
modelFit_rf$forest
modelFit_rf$test
modelFit_gbm$finalModel
confusionMatrix(vowel.test$y, predict(modelFit_gbm, vowel.test))
confusionMatrix(vowel.test$y, predict(modelFit_gbm, vowel.test$y))
confusionMatrix(vowel.test, predict(modelFit_gbm, vowel.test))
table(vowel.test$y, predict(modelFit_gbm, vowel.test))
summary(table(vowel.test$y, predict(modelFit_gbm, vowel.test)))
table(vowel.test$y, predict(modelFit_gbm, vowel.test))
postResample(vowel.test$y, predict(modelFit_gbm, vowel.test))
summary(vowel.test$y, predict(modelFit_gbm, vowel.test))
library(caret)
library(randomForest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
modelFit_rf <- randomForest(as.factor(y) ~., data=vowel.train)
modelFit_gbm <- train(as.factor(y) ~., data=vowel.train, method="gbm")
mean(vowel.test$y == predict(modelFit_rf, vowel.test))
mean(vowel.test$y == predict(modelFit_gbm, vowel.test))
modelFit_rf$confusion
modelFit_rf$confusion[, 'class.error']
confusionMatrix(vowel.test$y, predict(modelFit_rf, vowel.test))
confusionMatrix(predict(modelFit_rf, vowel.test), vowel.test$y)
modelFit_rf <- randomForest(as.factor(y) ~., data=vowel.train, metric = "Accuracy")
confusionMatrix(predict(modelFit_rf, vowel.test), vowel.test$y)
is.na(vowel.test)
sum(is.na(vowel.test))
sum(is.na(vowel.train))
confusionMatrix(predict(modelFit_rf, vowel.test), as.factor(vowel.test$y))
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelFit_rf <- randomForest(y ~., data=vowel.train)
confusionMatrix(predict(modelFit_rf, vowel.test), vowel.test$y)
set.seed(33833)
modelFit_rf <- randomForest(y ~., data=vowel.train)
modelFit_gbm <- train(y ~., data=vowel.train, method="gbm")
confusionMatrix(predict(modelFit_gbm, vowel.test), vowel.test$y)
set.seed(33833)
modelFit_rf <- randomForest(y ~., data=vowel.train)
modelFit_gbm <- train(y ~., data=vowel.train, method="gbm", verbose = FALSE)
confusionMatrix(predict(modelFit_gbm, vowel.test), vowel.test$y)
equalPredictions <- predict(modelFit_rf, vowel.test) == predict(modelFit_gbm, vowel.test)
summary(equalPredictions)
mean(equalPredictions)
319/(319+143)
?train
library(e1071)
accuracy()
accuracy
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
## Set up the model
modelFit_rf <- train(diagnosis ~., method="rf", data=training)
modelFit_gbm <- train(diagnosis ~., method="gbm", data=training, verbose = FALSE)
modelFit_lda <- train(diagnosis ~., method="lda", data=training)
## Accuracy
confusionMatrix(testing$diagnosis, predict(modelFit_rf, testing))
confusionMatrix(testing$diagnosis, predict(modelFit_gbm, testing))
confusionMatrix(testing$diagnosis, predict(modelFit_lda, testing))
library(lars)
lasso.fit <- lars(as.matrix(x), y, type="lasso", trace=TRUE)
png(file = "plot.png", width=432, height=432, pointsize=8)
plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))
dev.off()
# This plots the cross validation curve
lasso.cv <- cv.lars(as.matrix(x), y, K=10, type="lasso", trace=TRUE)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
head(training)
set.seed(233)
library(lars)
install.packages("glmnet")
library(lars)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
head(training)
str(training)
set.seed(233)
library(lars)
library(glmnet)
x <- as.matrix(training[, -9])
y <- as.double(as.matrix(training[, 9]))
cv.lasso <- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
shiny::runApp('Conferences, Competitions, Online Courses/Coursera/Developing Data Products/Machine_Learning')
# Setting up the university seal
library(leaflet)
DLSU_Icon <- makeIcon("https://upload.wikimedia.org/wikipedia/en/thumb/c/c2/De_La_Salle_University_Seal.svg/175px-De_La_Salle_University_Seal.svg.png", iconWidth = 31*215/230, iconHeight = 31, iconAnchorX = 31*215/230/2, iconAnchorY = 16)
# Coordinates of the Manila Campus
Campus_Map <- data.frame(Campus = c("Manila", "Laguna"), lat = c(14.5648,14.2626), long = c(120.9931,121.0429))
# Setting up the interactive map
Campus_Map %>%
leaflet() %>%
addTiles() %>%
addMarkers(icon = DLSU_Icon)
library(plotly)
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
data(mtcars)
library(plotly)
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
install.packages("plotly")
library(plotly)
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
library(plotly)
plot_ly(mtcars, x = ~wt, y = ~mpg)
?plot_ly
plot_ly(economics, x = ~date, y = ~pop)
head(economics)
head(economic)
library(ggplot2)
data("economics")
economics
plot_ly(x = ~pop, data = economics, type = "histogram")
library(plotly)
plot_ly(x = ~pop, data = economics, type = "histogram")
?plot_ly
plot_ly(x = ~pop, data = economics, type = "histogram")
head(economics)
plot_ly(x = ~date, y = ~unemploy, data = economics, type="scatter", mode = "lines")
?economics
table(economics$psavert)
plot_ly(x = ~pce, y = ~psavert, data = economics, type="scatter")
plot_ly(x = ~psavert, y = ~pce, data = economics, type="scatter")
p <- plot_ly(x = ~pce, y = ~psavert, data = economics, type="scatter") %>%
layout(title = "Personal Savings and Person Consumption Expenditure",
scene = list(
xaxis = list(title = "Personal Consumption Expenditure"),
yaxis = list(title = "Personal Average Savings Rate")
))
p
?layout
p <- plot_ly(x = ~pce, y = ~psavert, data = economics, type="scatter") %>%
layout(title = "Personal Savings and Person Consumption Expenditure",
xaxis = list(title = "Personal Consumption Expenditure"),
yaxis = list(title = "Personal Average Savings Rate")
)
p
plot_ly(x = ~psavert, y = ~pce, data = economics, type="scatter")
setwd("~/Conferences, Competitions, Online Courses/Coursera/Practical Machine Learning/PML")
